{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "26a77fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, ceil\n",
    "import csv\n",
    "\n",
    "def getOffsets(A, option): #normalizing an array\n",
    "    \n",
    "    _,m=A.shape\n",
    "    \n",
    "    #lower and upper bounds joint angles\n",
    "    lb_theta = np.array([0,-30,-30,-180,-90,-180])*np.pi/180;   \n",
    "    ub_theta = np.array([90,60,30,180,90,180])*np.pi/180;     \n",
    "    df_theta = ub_theta-lb_theta\n",
    "    \n",
    "    #lower and upper bounds force\n",
    "    lb_force = np.array([-2000,-2000,-2000,0,0,0]); ub_force = np.array([2000,2000,2000,0,0,0]); df_force=ub_force-lb_force\n",
    "    \n",
    "    #lower and upper bounds of absolute pose xyz coordinates\n",
    "    lb_pose = np.array([-1,-1,-1]); ub_pose = np.array([3,3,3]); df_pose=ub_pose-lb_pose          \n",
    "    \n",
    "    #lower and upper bounds of quaternions\n",
    "    lb_quat=np.array([-1,-1,-1,-1]); ub_quat=np.array([1,1,1,1]); df_quat=ub_quat-lb_quat\n",
    "    \n",
    "    # relative pose and angular error shall not be normalized\n",
    "    offset_z = np.array([0,0,0]); length_z=np.array([1,1,1])\n",
    "    \n",
    "    if (option==\"minmax\"):\n",
    "        offset=[np.min(A[:,i:i+1]) for i in range(m)]\n",
    "        length=[np.max(A[:,i:i+1])-np.min(A[:,i:i+1]) for i in range(m)]\n",
    "    \n",
    "    elif(option==\"standard\"):\n",
    "        offset=[np.mean(A[:,i:i+1]) for i in range(m)]\n",
    "        length=[np.std(A[:,i:i+1]) for i in range(m)]        \n",
    "    \n",
    "    elif(option==\"dataset1\"): #dataset 1 \n",
    "        # input (6): six joint angles (rad); \n",
    "        # output (7): absolute pose xyz, i.e. position (m) and quaternions\n",
    "        offset=np.concatenate((lb_theta, lb_pose,lb_quat))\n",
    "        length=np.concatenate((df_theta, df_pose, df_quat))\n",
    "    \n",
    "    elif(option==\"dataset2\"): #dataset 2 \n",
    "        # input (12): six joint angles (rad), wrench vector (force(N), torque(Nm)=zeros)\n",
    "        # output (7): absolute pose, i.e. position (m) and quaternions\n",
    "        offset = np.concatenate((lb_theta, lb_force, lb_pose, lb_quat))\n",
    "        length = np.concatenate((df_theta, df_force, df_pose, df_quat))        \n",
    "\n",
    "    elif(option==\"dataset3\"): #dataset 3 \n",
    "        # input (12): six joint angles (rad), wrench vector (force(N), torque(Nm)=zeros)\n",
    "        # output (7): relative pose, i.e. position error (m) and quaternion error\n",
    "        offset = np.concatenate((lb_theta, lb_force, offset_z, lb_quat))\n",
    "        length = np.concatenate((df_theta, df_force, length_z, df_quat))                \n",
    "        \n",
    "    elif(option==\"dataset4\"): #dataset 4 \n",
    "        # input (6): six joint angles (rad)\n",
    "        # intput (7): absolute pose, i.e. position (m) and quaternions\n",
    "        offset = np.concatenate((lb_theta, lb_pose, lb_quat))\n",
    "        length = np.concatenate((df_theta, df_pose, df_quat))            \n",
    "        \n",
    "    elif(option==\"dataset5\"): #dataset 5 \n",
    "        # input (6): six joint angles (rad)\n",
    "        # output (6): relative pose, i.e. position error (m) and angular error (rad)\n",
    "        offset = np.concatenate((lb_theta, offset_z, offset_z))\n",
    "        length = np.concatenate((df_theta, length_z, length_z))\n",
    "\n",
    "    elif(option==\"dataset6\"): #dataset 6 \n",
    "        # input (9): six joint angles (rad), force vector (N)\n",
    "        # output (6): absolute pose, i.e. position (m) and angular error (rad)\n",
    "        offset=np.concatenate((lb_theta, lb_force[:3],lb_pose,offset_z))\n",
    "        length=np.concatenate((df_theta, df_force[:3],df_pose,length_z))        \n",
    "    \n",
    "    elif(option==\"none\"): #no normalization\n",
    "        offset=np.zeros(m)\n",
    "        length=np.zeros(m)\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: unknown option for normalization\")                \n",
    "        raise Exception(\"\")\n",
    "        \n",
    "    return(offset,length)\n",
    "\n",
    "\n",
    "def normalize(A,offset,length):\n",
    "    _,m=A.shape\n",
    "    for i in range(m):\n",
    "        if(length[i]!=0):\n",
    "            A[:,i:i+1] = (A[:,i:i+1]-offset[i])/length[i]        \n",
    "\n",
    "        \n",
    "def denormalize(A,offset,length): #normalizing an array\n",
    "    _,m=A.shape\n",
    "    for i in range(m):\n",
    "        if(length[i]!=0):\n",
    "            A[:,i:i+1] = A[:,i:i+1]*length[i] + offset[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c8b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(fn=\"dataset5\"):\n",
    "    dataframe = np.loadtxt(\"datasets/\"+fn+\".csv\",delimiter=\",\")\n",
    "    m,N=dataframe.shape\n",
    "    \n",
    "    #normalization\n",
    "    offset, length = getOffsets(dataframe,fn)\n",
    "    inputs=6\n",
    "\n",
    "    normalize(dataframe, offset,length)\n",
    "    \n",
    "    #data splitting\n",
    "    \n",
    "    data = dataframe[:,:inputs]\n",
    "    labels= dataframe[:,inputs:]\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2,shuffle=True)\n",
    "\n",
    "# random_state=42\n",
    "    #create model\n",
    "    model=ELM_model(data_train,labels_train,252,distribution=\"normal\",activation=\"sigmoid\")\n",
    "\n",
    "    print(model.analyse_mean_error_XYZ(data_train,labels_train,offset,length))\n",
    "    print(model.analyse_mean_error_XYZ(data_test,labels_test,offset,length))    \n",
    "\n",
    "    return(model, data_test,labels_test)\n",
    "    \n",
    "#    model.print_prediction(data_train,labels_train,offset,length,xlim=0.2)\n",
    "    \n",
    "#    print(model.analyse_mean_error_XYZ(data_test[10:11],labels_test[10:11],offset,length))\n",
    "#    model.learnBatch(data_test,labels_test,offset,length)\n",
    "#    model.print_prediction(data_train,labels_train,offset,length,xlim=0.2)\n",
    "#learnBatch(model,data_train,labels_train,offset,length)\n",
    "#learnBatch(model,data,labels,offset,length)\n",
    "#print(model.analyse_mean_error_XYZ(data,labels,offset,length))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f418ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def error_prediction(filename, inputs=9, normalization=\"standard\", neurons=100, activation=\"relu\",distribution=\"rand\",xlim=10,n_bins=400):\n",
    "    dataframe = np.loadtxt(filename,delimiter=\",\")\n",
    "    m,N=dataframe.shape\n",
    "    \n",
    "    #normalization\n",
    "    offset, length = getOffsets(dataframe, normalization)\n",
    "    \n",
    "    normalize(dataframe,offset,length)\n",
    "    \n",
    "    #data splitting\n",
    "    data = dataframe[:,:inputs]\n",
    "    labels= dataframe[:,inputs:]\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.20, random_state=42)\n",
    "\n",
    "    #create model\n",
    "    model=ELM_model(data_train,labels_train,neurons,activation,distribution)\n",
    "\n",
    "    #predict\n",
    "    prediction=model.predict(data_test)\n",
    "\n",
    "    #denormalization    \n",
    "    denormalize(prediction, offset, length)\n",
    "    denormalize(labels_test, offset, length)\n",
    "\n",
    "    diff=prediction-labels_test\n",
    "    \n",
    "    normalize(labels_test,offset,length)\n",
    "    return(diff)\n",
    "\n",
    "def print_prediction(filename, inputs=9, normalization=\"standard\", neurons=100, activation=\"relu\",distribution=\"rand\", xlim=10,n_bins=400):\n",
    "    \n",
    "    diff = error_prediction(filename, inputs, normalization, neurons, activation,distribution,xlim,n_bins)                                   \n",
    "    \n",
    "    # Convert from m in mm\n",
    "    diff=diff*1000\n",
    "    \n",
    "    figure,axis=plt.subplots(1,3)\n",
    "    figure.set_figwidth(50)\n",
    "    figure.set_figheight(20)        \n",
    "    maximum={}\n",
    "    mean={}\n",
    "    stderiv={}\n",
    "    XLabels=[\"X error in mm\", \"Y error in mm\", \"Z error in mm\"]    \n",
    "    for i in range(3):        \n",
    "        maximum[i]=np.max(abs(diff[:,i:i+1]))\n",
    "        mean[i]=np.mean(diff[:,i:i+1])\n",
    "        stderiv[i]=np.std(diff[:,i:i+1])\n",
    "        axis[i].hist(abs(diff[:,i:i+1]),n_bins)\n",
    "        axis[i].set_xlim(0,xlim)        \n",
    "        axis[i].set_xlabel(XLabels[i])        \n",
    "    plt.show()     \n",
    "    label=[\"X\",\"Y\",\"Z\"]\n",
    "    print('{:<25}{:^25}{:>25}'.format(\"Maximum\",\"Mean\",\"StDerv\"))\n",
    "    print('{:<25}{:^25}{:>25}'.format(\"-----\",\"-----\",\"-----\"))    \n",
    "    for i in range(3):\n",
    "        print(label[i],'{:<25}{:^25}{:>25}'.format(str(maximum[i]),str(mean[i]),str(stderiv[i])))\n",
    "        \n",
    "    absolute=[np.sqrt(diff[i,0]**2 + diff[i,1]**2 + diff[i,2]**2) for i in range(len(diff))]\n",
    "    print(\"Average absolute positional error in mm: \", np.average(absolute))\n",
    "\n",
    "def loss(filename, inputs=9, normalization=\"standard\", neurons=100, activation=\"relu\",distribution=\"rand\",xlim=10,n_bins=400):\n",
    "    diff = error_prediction(filename, inputs, normalization, neurons, activation,distribution,xlim,n_bins)*1000\n",
    "    absolute=[np.sqrt(diff[i,0]**2 + diff[i,1]**2 + diff[i,2]**2) for i in range(len(diff))]\n",
    "    return(np.mean(absolute))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f0e6af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f [model, x ,y]\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, ceil\n",
    "import csv\n",
    "\n",
    "SMALLEST = np.nextafter(0, 1)\n",
    "\n",
    "class ELM_model:\n",
    "    \n",
    "    def __init__(self,data,labels,hidden_size,activation=\"relu\",distribution=\"rand\",norandom=False):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.hidden_size=hidden_size\n",
    "        if(activation==\"relu\"):\n",
    "            self.activation=self.relu\n",
    "        elif(activation==\"sigmoid\"):\n",
    "            self.activation=self.sigmoid\n",
    "        else:\n",
    "            print(\"Error: no such activation rule implemented\")\n",
    "            return()\n",
    "        \n",
    "        # set input nodes randomly either using rand or normal distribution\n",
    "        _, input_size = self.data.shape\n",
    "        if(norandom):\n",
    "            self.input_weights = np.ones((input_size,hidden_size))\n",
    "            self.biases = (-1)*np.ones((hidden_size))\n",
    "        else:            \n",
    "            if(distribution==\"rand\"):\n",
    "                self.input_weights = np.random.rand(input_size,hidden_size)*2-1\n",
    "                self.biases = np.random.rand(hidden_size)*2-1\n",
    "            elif(distribution==\"normal\"):\n",
    "                self.input_weights = np.random.normal(size=[input_size,hidden_size])\n",
    "                self.biases = np.random.normal(size=[hidden_size])\n",
    "            else:\n",
    "                raise Exception(\"Error: unknown distribution option\")            \n",
    "        \n",
    "        # set hidden neurons\n",
    "        self.H = self.hidden_nodes(self.data)\n",
    "        # using SG-article\n",
    "        self.M = np.linalg.pinv(np.dot(self.H.transpose(),self.H))\n",
    "        self.output_weights=np.dot(np.linalg.pinv(self.H),self.labels)\n",
    "        # set Moore Penrose inverse for sequential learning\n",
    "        # self.H_MP = np.linalg.pinv(self.H)\n",
    "        # self.output_weights = np.dot(self.H_MP, self.labels)            \n",
    "  \n",
    "    def hidden_nodes(self,X):\n",
    "        G = np.dot(X, self.input_weights)\n",
    "        G = G + self.biases\n",
    "        H=self.activation(G)\n",
    "        return H\n",
    "  \n",
    "    def predict(self,X):\n",
    "        out = self.hidden_nodes(X)\n",
    "        out = np.dot(out, self.output_weights)\n",
    "        return out        \n",
    "            \n",
    "    def relu(self,x):\n",
    "       return np.maximum(x, 0, x)\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def dump(self):\n",
    "        print(\"labels=\", self.labels)\n",
    "        print(\"H=\", self.H)\n",
    "        print(\"MP=\", self.H_MP)\n",
    "        print(\"output_weights=\", self.output_weights)\n",
    "    \n",
    "    def sequential_learning(self, x, y):                \n",
    "#        a=self.hidden_nodes(x)\n",
    "#        A=self.H\n",
    "#        Anew = np.vstack((A,a))        \n",
    "#\n",
    "#        Ap=self.H_MP\n",
    "#        #Hnew = (self.recursiveMP(A.transpose(),a.transpose(),Ap.transpose())).transpose()\n",
    "#        Hnew = np.linalg.pinv(Anew)\n",
    "#                \n",
    "#        ynew = np.vstack((self.labels,y))\n",
    "#        self.output_weights = np.dot(Hnew,ynew)\n",
    "    # update rule by SG\n",
    "        M = self.M\n",
    "        H = self.hidden_nodes(x)\n",
    "        Ht = H.transpose()\n",
    "        Block=H.shape[0]\n",
    "        Aux =  np.linalg.pinv( np.eye(Block) + np.dot(np.dot(H,M),Ht))\n",
    "        M = M - np.dot(np.dot(np.dot(np.dot(M,Ht),Aux), H), M); \n",
    "        beta = self.output_weights\n",
    "        beta = beta + np.dot(np.dot(M,Ht), (y - np.dot(H,beta)))\n",
    "        self.output_weights=beta\n",
    "        self.M = M                \n",
    "\n",
    "    def recursiveMP(self,A,a,Ap):\n",
    "        n,m=A.shape\n",
    "        a=a.reshape(n,1)\n",
    "        d = np.dot(Ap,a)    \n",
    "        c = a - np.dot(A,d)\n",
    "        \n",
    "        if(np.all((c==0))):\n",
    "            b= 1/np.dot(c.transpose(),c)*c.transpose()        \n",
    "        else:\n",
    "            k=1/(1+np.dot(d.transpose(),d))            \n",
    "            b=k*np.dot(d.transpose(),Ap)\n",
    "      \n",
    "        return(np.vstack((Ap-np.dot(d,b),b)))         \n",
    "    \n",
    "    def analyse_mean_error_XYZ(self, X,Y,offset, length):        \n",
    "        prediction=self.predict(X)\n",
    "        denormalize(prediction, offset, length)\n",
    "        denormalize(Y, offset, length)\n",
    "        diff=(prediction-Y)*1000\n",
    "        absolute=[np.sqrt(diff[i,0]**2 + diff[i,1]**2 + diff[i,2]**2) for i in range(len(diff))]\n",
    "        \n",
    "        normalize(Y,offset,length)\n",
    "        return(np.mean(absolute))\n",
    "    \n",
    "    def MSELoss(self,X,Y):\n",
    "        prediction=self.predict(X)        \n",
    "        diff=(prediction-Y)\n",
    "        return(1/len(diff)*sum([np.linalg.norm(x)**2 for x in diff]))\n",
    "        \n",
    "        \n",
    "    def learnBatch(self, X,Y,o,l):\n",
    "        print(\"Learn Batch:\")\n",
    "        print(\" Error mean before:\\t\", self.analyse_mean_error_XYZ(X,Y,o,l))\n",
    "        for i in range(len(X)):\n",
    "            x=np.array([X[i]])\n",
    "            y=np.array([Y[i]])\n",
    "            self.sequential_learning(x,y)\n",
    "        print(\" Error mean after:\\t\", self.analyse_mean_error_XYZ(X,Y,o,l))    \n",
    "\n",
    "    def print_prediction(self, X,Y,o,l, xlim=10,n_bins=400):\n",
    "        prediction=self.predict(X)\n",
    "        denormalize(prediction, o,l)\n",
    "        denormalize(Y, o,l)\n",
    "        diff=(prediction-Y)*1000\n",
    "    \n",
    "        figure,axis=plt.subplots(1,3)\n",
    "        figure.set_figwidth(50)\n",
    "        figure.set_figheight(20)        \n",
    "        maximum={}\n",
    "        mean={}\n",
    "        stderiv={}\n",
    "        XLabels=[\"X error in mm\", \"Y error in mm\", \"Z error in mm\"]    \n",
    "        for i in range(3):        \n",
    "            maximum[i]=np.max(abs(diff[:,i:i+1]))\n",
    "            mean[i]=np.mean(diff[:,i:i+1])\n",
    "            stderiv[i]=np.std(diff[:,i:i+1])\n",
    "            axis[i].hist(abs(diff[:,i:i+1]),n_bins)\n",
    "            axis[i].set_xlim(0,xlim)        \n",
    "            axis[i].set_xlabel(XLabels[i])        \n",
    "        plt.show()     \n",
    "        label=[\"X\",\"Y\",\"Z\"]\n",
    "        print('{:<25}{:^25}{:>25}'.format(\"Maximum\",\"Mean\",\"StDerv\"))\n",
    "        print('{:<25}{:^25}{:>25}'.format(\"-----\",\"-----\",\"-----\"))    \n",
    "        for i in range(3):\n",
    "            print(label[i],'{:<25}{:^25}{:>25}'.format(str(maximum[i]),str(mean[i]),str(stderiv[i])))\n",
    "        \n",
    "        absolute=[np.sqrt(diff[i,0]**2 + diff[i,1]**2 + diff[i,2]**2) for i in range(len(diff))]\n",
    "        print(\"Average absolute positional error in mm: \", np.average(absolute))\n",
    "\n",
    "####################### ELM ende########################\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def testit(neurons, freq=2, N=100, shuffle=True, printing=False,norandom=False):\n",
    "    plotx = np.arange(N)*(1/N)\n",
    "    ploty = np.sin(freq * np.pi * plotx)\n",
    "    plotx.resize((N,1))\n",
    "    ploty.resize((N,1))\n",
    "    x=plotx.copy()\n",
    "    y=ploty.copy()\n",
    "    x.resize((N,1))\n",
    "    y.resize((N,1))\n",
    "    \n",
    "    xtr, xtest, ytr, ytest = train_test_split(x, y, test_size=0.2,shuffle=shuffle)\n",
    "\n",
    "    # 20% für Testdaten zum validieren.\n",
    "    # Anlernen 20% der Trainingsdaten \n",
    "    # sequentielles Lernen bis auf 80%\n",
    "\n",
    "    initN=int(N*0.2)\n",
    "    testN=int(N*0.6)\n",
    "\n",
    "    xtr_init = xtr[:initN]\n",
    "    ytr_init=ytr[:initN]\n",
    "    \n",
    "    m=ELM_model(xtr_init,ytr_init,neurons,activation=\"sigmoid\",norandom=norandom)    \n",
    "\n",
    "    \n",
    "    A = m.MSELoss(xtest,ytest)\n",
    "\n",
    "    MSELoss=[]\n",
    "    return((m,xtr,ytr,xtest,ytest,initN,testN))\n",
    "           \n",
    "    #for i in trange(testN, desc=\"learning\"):            \n",
    "    for i in range(testN):            \n",
    "            xx=np.array([xtr[initN+i]])\n",
    "            yy=np.array([ytr[initN+i]])\n",
    "            m.sequential_learning(xx,yy)           \n",
    "            MSELoss.append(m.MSELoss(xtest,ytest))\n",
    "    \n",
    "    B=m.MSELoss(xtest,ytest)\n",
    "    \n",
    "    if(printing):\n",
    "        print(\"Loss-Function\")\n",
    "        xaxis=np.linspace(0,testN - 1,testN)    \n",
    "        plt.plot(xaxis,np.array(MSELoss))\n",
    "        plt.show()\n",
    "        print(\"Prediction\")\n",
    "        prediction = m.predict(plotx)\n",
    "        diff = abs(prediction-ploty)\n",
    "        figure,axis=plt.subplots(1,2)\n",
    "        figure.set_figwidth(50)\n",
    "        figure.set_figheight(15)    \n",
    "        axis[0].plot(plotx, ploty,color=\"blue\")\n",
    "        axis[0].plot(plotx, prediction,color=\"red\")\n",
    "        axis[1].plot(plotx, diff,color=\"green\")\n",
    "        plt.show()\n",
    "    print((abs(B-A)))\n",
    "    #return(m,plotx,ploty)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b1e4bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,trainD,trainL,testD,testL,iN,tN=testit(20,freq=2,N=100,shuffle=True,printing=False,norandom=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0ec181f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1897737628970927e-06\n",
      "80 20\n",
      "(array([0.57]), array([-0.42577929]))\n"
     ]
    }
   ],
   "source": [
    "print(model.MSELoss(testD,testL))\n",
    "print(len(trainD),iN)\n",
    "print((np.array(trainD[iN]),np.array(trainL[iN])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "81a9b7a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,40,20) and (20,40,1) not aligned: 20 (dim 2) != 40 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [285]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequential_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrainD\u001b[49m\u001b[43m[\u001b[49m\u001b[43miN\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrainL\u001b[49m\u001b[43m[\u001b[49m\u001b[43miN\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mMSELoss(testD,testL))\n",
      "Input \u001b[1;32mIn [270]\u001b[0m, in \u001b[0;36mELM_model.sequential_learning\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     87\u001b[0m Ht \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m     88\u001b[0m Block\u001b[38;5;241m=\u001b[39mH\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 89\u001b[0m Aux \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv( np\u001b[38;5;241m.\u001b[39meye(Block) \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mHt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     90\u001b[0m M \u001b[38;5;241m=\u001b[39m M \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mdot(M,Ht),Aux), H), M); \n\u001b[0;32m     91\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_weights\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,40,20) and (20,40,1) not aligned: 20 (dim 2) != 40 (dim 1)"
     ]
    }
   ],
   "source": [
    "model.sequential_learning(np.array([trainD[iN:tN]]),np.array([trainL[iN:tN]]))\n",
    "print(model.MSELoss(testD,testL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9eec0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainDataSet(i,neurons,distribution,learn=0,norandom=False):\n",
    "    Datasets_input=[0,6,12,12,6,6,9]\n",
    "    n_bins=400\n",
    "    inputs=Datasets_input[i]\n",
    "    dataset=\"dataset\"+str(i)\n",
    "    filename=\"datasets/\"+dataset+\".csv\"        \n",
    "\n",
    "    dataframe = np.loadtxt(filename,delimiter=\",\")\n",
    "    m,N=dataframe.shape\n",
    "    \n",
    "    #normalization\n",
    "    offset, length = getOffsets(dataframe, dataset)\n",
    "    \n",
    "    normalize(dataframe,offset,length)\n",
    "    \n",
    "    #data splitting\n",
    "    data = dataframe[:,:inputs]\n",
    "    labels= dataframe[:,inputs:]\n",
    "    \n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.20, shuffle=True)\n",
    "\n",
    "    #create model\n",
    "    #use only 20% of trainings data\n",
    "    initN=int(len(data_train)*0.2)\n",
    "    trainN=len(data_train)-initN\n",
    "    \n",
    "        \n",
    "    model=ELM_model(data_train[:initN],labels_train[:initN],neurons,\"sigmoid\",distribution,norandom=norandom)\n",
    "\n",
    "    A=model.MSELoss(data_test,labels_test)\n",
    "    #predict\n",
    " #   prediction=model.predict(data_test)\n",
    "\n",
    " #  #denormalization    \n",
    " #  denormalize(prediction, offset, length)\n",
    " #  denormalize(labels_test, offset, length)\n",
    " #  diff=prediction-labels_test    \n",
    " #  normalize(labels_test,offset,length)\n",
    " #   \n",
    " #  # Convert from m in mm\n",
    " #  diff=diff*1000    \n",
    " #  figure,axis=plt.subplots(1,3);    figure.set_figwidth(50);    figure.set_figheight(20)        \n",
    " #  maximum, mean, stderiv = {}, {}, {}    \n",
    " #  XLabels=[\"X error in mm\", \"Y error in mm\", \"Z error in mm\"]    \n",
    " #  for i in range(3):        \n",
    " #      maximum[i]=np.max(abs(diff[:,i:i+1]))\n",
    " #      mean[i]=np.mean(diff[:,i:i+1])\n",
    " #      stderiv[i]=np.std(diff[:,i:i+1])\n",
    " #      axis[i].hist(abs(diff[:,i:i+1]),n_bins)\n",
    " #      axis[i].set_xlim(0,xlim)        \n",
    " #      axis[i].set_xlabel(XLabels[i])        \n",
    " #  plt.show()     \n",
    " #  label=[\"X\",\"Y\",\"Z\"]\n",
    " #  print('{:<25}{:^25}{:>25}'.format(\"Maximum\",\"Mean\",\"StDerv\"))\n",
    " #  print('{:<25}{:^25}{:>25}'.format(\"-----\",\"-----\",\"-----\"))    \n",
    " #  for i in range(3):\n",
    " #      print(label[i],'{:<25}{:^25}{:>25}'.format(str(maximum[i]),str(mean[i]),str(stderiv[i])))\n",
    " #      \n",
    " #  absolute=[np.sqrt(diff[i,0]**2 + diff[i,1]**2 + diff[i,2]**2) for i in range(len(diff))]\n",
    " #  print(\"Average absolute positional error in mm: \", np.average(absolute))\n",
    " #  \n",
    " # \n",
    " #  for i in range(len(data_test)):\n",
    " #          xx=np.array([data_test[i]])\n",
    " #          yy=np.array([labels_test[i]])\n",
    " #          model.sequential_learning(xx,yy)\n",
    "    if(learn>0):        \n",
    "        MSELoss=[]        \n",
    "        for i in trange(trainN, desc=\"learning\"):                    \n",
    "            xx=np.array([data_train[initN+i]])\n",
    "            yy=np.array([labels_train[initN+i]])\n",
    "            model.sequential_learning(xx,yy)           \n",
    "            if(i%(int(learn*trainN))==0):\n",
    "                MSELoss.append(model.MSELoss(data_test,labels_test))              \n",
    "                \n",
    "        print(\"Loss-Function\")\n",
    "        xaxis=np.linspace(0, len(MSELoss)-1, int(len(MSELoss)))\n",
    "    \n",
    "        plt.plot(xaxis,np.array(MSELoss))\n",
    "        plt.show()\n",
    "    \n",
    "    print(A,\"\\t\",model.MSELoss(data_test,labels_test))\n",
    "    print(\"Overall Loss:\", model.MSELoss(data,labels))\n",
    "#    prediction=model.predict(data_test)\n",
    "\n",
    "#  #denormalization    \n",
    "#  #denormalize(prediction, offset, length)\n",
    "#  #denormalize(labels_test, offset, length)\n",
    "#  diff=prediction-labels_test    \n",
    "#  #normalize(labels_test,offset,length)\n",
    "#   \n",
    "#  # Convert from m in mm\n",
    "#  #diff=diff*1000    \n",
    "#  figure,axis=plt.subplots(1,3);    figure.set_figwidth(50);    figure.set_figheight(20)        \n",
    "#  maximum, mean, stderiv = {}, {}, {}    \n",
    "#  XLabels=[\"X error in mm\", \"Y error in mm\", \"Z error in mm\"]    \n",
    "#  for i in range(3):        \n",
    "#      maximum[i]=np.max(abs(diff[:,i:i+1]))\n",
    "#      mean[i]=np.mean(diff[:,i:i+1])\n",
    "#      stderiv[i]=np.std(diff[:,i:i+1])\n",
    "#      axis[i].hist(abs(diff[:,i:i+1]),n_bins)\n",
    "#      axis[i].set_xlim(0,xlim)        \n",
    "#      axis[i].set_xlabel(XLabels[i])        \n",
    "#  plt.show()     \n",
    "#  label=[\"X\",\"Y\",\"Z\"]\n",
    "#  print('{:<25}{:^25}{:>25}'.format(\"Maximum\",\"Mean\",\"StDerv\"))\n",
    "#  print('{:<25}{:^25}{:>25}'.format(\"-----\",\"-----\",\"-----\"))    \n",
    "#  for i in range(3):\n",
    "#      print(label[i],'{:<25}{:^25}{:>25}'.format(str(maximum[i]),str(mean[i]),str(stderiv[i])))\n",
    "#      \n",
    "#  absolute=[np.linalg.norm(diff[i])**2  for i in range(len(diff))]\n",
    "#  print(\"Average  error: \", np.average(absolute))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9e8cdd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff587e393d654e36823f19887b0d94a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "learning:   0%|          | 0/6400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss-Function\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11135933228043327 \t 0.06703194334122067\n",
      "Overall Loss: 0.059923444750565574\n"
     ]
    }
   ],
   "source": [
    "trainDataSet(1,500,\"normal\", learn=0.3,norandom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.2*123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656484f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
